{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYQ6Lh11LFkP"
   },
   "source": [
    "# Workshop - Przetwarzanie języka polskiego ze spaCy, 26 października 2020\n",
    "\n",
    "NLPDay 2020\n",
    "\n",
    "Ryszard Tuora\n",
    "\n",
    "\n",
    "2 modele do języka polskiego w spaCy:\n",
    "\n",
    "IPI PAN - bardziej rozbudowany, wolniejszy, bardziej złożona instalacja - http://zil.ipipan.waw.pl/SpacyPL\n",
    "\n",
    "model oficjalny - prostszy, znacznie szybszy, prosta instalacja - https://spacy.io/models/pl\n",
    "\n",
    "model IPI PAN dla języka polskiego\n",
    "\n",
    "składa się z:\n",
    "- taggera morfosyntaktycznego\n",
    "- lematyzatora\n",
    "- parsera zależnościowego\n",
    "- komponentu NER\n",
    "- flexera (komponentu do fleksji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RprQFxeM4zas"
   },
   "outputs": [],
   "source": [
    "# Przygotowanie środowiska, komendy linux\n",
    "# instalacja Morfeusza 2\n",
    "!wget -O - http://download.sgjp.pl/apt/sgjp.gpg.key|sudo apt-key add -\n",
    "!sudo apt-add-repository http://download.sgjp.pl/apt/ubuntu\n",
    "!sudo apt update\n",
    "!sudo apt install morfeusz2\n",
    "!sudo apt install python3-morfeusz2\n",
    "\n",
    "\n",
    "# instalacja spaCy\n",
    "\n",
    "!python3 -m pip install spacy\n",
    "\n",
    "# 1. instalacja modelu IPI PAN dla języka polskiego\n",
    "!wget \"http://zil.ipipan.waw.pl/SpacyPL?action=AttachFile&do=get&target=pl_spacy_model_morfeusz-0.1.3.tar.gz\"\n",
    "!mv 'SpacyPL?action=AttachFile&do=get&target=pl_spacy_model_morfeusz-0.1.3.tar.gz' pl_spacy_model_morfeusz-0.1.3.tar.gz\n",
    "!python3 -m pip install pl_spacy_model_morfeusz-0.1.3.tar.gz\n",
    "\n",
    "# linkowanie modelu do spaCy\n",
    "!python3 -m spacy link pl_spacy_model_morfeusz pl_spacy_model_morfeusz -f\n",
    "\n",
    "# 2. instalacja oficjalnego modelu spaCy\n",
    "!python3 -m spacy download pl_core_news_lg\n",
    "\n",
    "# dodatkowe zależności:\n",
    "!python3 -m pip install tqdm\n",
    "!python3 -m pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PtEk9Bt0N3u1"
   },
   "outputs": [],
   "source": [
    "### PYTHON 3\n",
    "# ładowanie modelu\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "nlp = spacy.load(\"pl_spacy_model_morfeusz\") # IPI PAN\n",
    "#nlp = spacy.load(\"pl_core_news_lg\") # OFICJALNY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zquQAX2HGVpg"
   },
   "source": [
    "# Część zero - Tokenizacja i reprezentacja tekstów\n",
    "\n",
    "Wejściem do modelu są łańcuchy tekstowe (stringi), na wyjściu dostajemy obiekt Doc reprezentujący struktury wykryte w tekście przez pełen potok (pipeline). Pierwszym krokiem który musi być wykonany aby przetworztć tekst, jest tokenizacja, czyli podział tekstu na tokeny/segmenty. Tokeny w większości przypadków odpowiadają słowom \"od spacji do spacji\". Ale warto zwrócić uwagę na kilka przypadków odstających od takiej prostej reguły.\n",
    "\n",
    "Wynikiem potoku spaCy jest obiekt Doc, który składa się z obiektów Token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrMbeDcfHIvu"
   },
   "outputs": [],
   "source": [
    "txt = \"Chciałby, żebym pojechał do miasta z zielono-żółto-białą flagą (np. Zielonej Góry).\"\n",
    "split = txt.split()\n",
    "doc = nlp(txt)\n",
    "print(\"    spaCy     vs.   .split()\\n\")\n",
    "for i in range(max([len(split), len(doc)])):\n",
    "    try:\n",
    "        tok1 = doc[i]\n",
    "    except IndexError:\n",
    "        tok1 = \"\"\n",
    "    try:\n",
    "        tok2 = split[i]\n",
    "    except IndexError:\n",
    "        tok2 = \"\"\n",
    "    print(\"{0:2}. {1:15} {2:15}\".format(i, tok1.orth_, tok2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy pozwala na zapisywanie przetworzonych dokumentów w formacie JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "doc_json = doc.to_json()\n",
    "json_string = json.dumps(doc_json, indent=2, ensure_ascii=False)\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4a5Izt4Tsae"
   },
   "source": [
    "# Część pierwsza - reprezentacje wektorowe\n",
    "\n",
    "Sercem modelu jest reprezentacja wektorowa słów, 500 tys. wektorów o długości 100 wyekstrahowanych z embeddingów word2vec KGR10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwvGXnaON7CB"
   },
   "source": [
    "### Bardzo popularny przykład \"arytmetyki słów\":\n",
    "znaczenie słów jest reprezentowane przez wektory, dla których mamy zdefiniowane operacje matematyczne. Możemy więc odjąć od znaczenia słowa \"królowa\", znaczenie słowa \"kobieta\", i dodać doń znaczenie słowa \"mężczyzna\", licząc iż wektor będący wynikiem takiego działania odpowiada słowu \"król\". W praktyce wektor taki najprawdopodobniej nie ma interpretacji, ale możemy znaleźć najbliższy wektor który ma jakąkolwiek interpretację przeszukując słownik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# od wersji 2.3 wektory są ładowane \"leniwie\"\n",
    "# musimy więc najpierw wymusić załadowanie wszystkich wektorów\n",
    "print(len(nlp.vocab))\n",
    "for s in nlp.vocab.vectors:\n",
    "    _ = nlp.vocab[s]\n",
    "print(len(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jb9ZXk4a8ety"
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    " \n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    " \n",
    "man = nlp.vocab['mężczyzna'].vector\n",
    "woman = nlp.vocab['kobieta'].vector\n",
    "queen = nlp.vocab['królowa'].vector\n",
    "king = nlp.vocab['król'].vector\n",
    " \n",
    "\n",
    "maybe_king = man - woman + queen\n",
    "computed_similarities = []\n",
    " \n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors\n",
    "    if not word.has_vector:\n",
    "        continue \n",
    "    similarity = cosine_similarity(maybe_king, word.vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    " \n",
    "sorted_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "print([w[0].text for w in sorted_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXJB6l6IIRJX"
   },
   "outputs": [],
   "source": [
    "w1 = \"pies\"\n",
    "w2 = \"psami\"\n",
    "w3 = \"zwierzę\"\n",
    "w4 = \"buldog\"\n",
    "w5 = \"obroża\"\n",
    "w6 = \"smycz\"\n",
    "w7 = \"marchewka\"\n",
    "w8 = \"słońce\"\n",
    "\n",
    "def similarity(w1, w2):\n",
    "    w1_lex = nlp.vocab[w1]\n",
    "    w2_lex = nlp.vocab[w2]\n",
    "    if w1_lex.has_vector and w2_lex.has_vector:\n",
    "        sim = w1_lex.similarity(w2_lex)\n",
    "        print(\"{} vs. {} -> {}\".format(w1, w2, sim))\n",
    "    else:\n",
    "        print(\"Jedno ze słów nie ma reprezentacji wektorowej.\")\n",
    "\n",
    "for w in [w2, w3, w4, w5, w6, w7, w8]:\n",
    "    similarity(w1, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SKbMLgI7GKd"
   },
   "source": [
    "## Zadanie 1:\n",
    "\n",
    "nlp.vocab może być traktowany jako iterator: znajdują się w nim obiekty klasy lexeme, nie wszystkim obiektom lexeme przypisane są wektory - można to poznać po atrybucie .has_vector. Napisz funkcję thesaurus(word) która dla podanego słowa znajduje dziesięć najbardziej podobnych słów. Możesz skorzystać z metody .similarity zdefiniowanej dla obiektów lexeme, która jako argument przyjmuje drugi obiekt lexeme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMw2hDepJ9EM"
   },
   "source": [
    "### spaCy umożliwia liczenie podobieństwa między słowami \n",
    "jest ono proporcjonalne do cosinusa kąta wektorami je reprezentującymi. Odpowiednia funkcja jest zdefiniowana dla leksemów (tutaj oznaczają one słowa z pominięciem kontekstu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-i6ko6L67Fpa"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy konstruuje także wektory dla zdań, i dokumentów. Wektor dla dokumentu jest dostępny w atrybucie .vector obiektu Doc, i jest równy uśrednionemu wektorowi tokenów z tego dokumentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector = doc.vector\n",
    "mean_document_vector = sum([tok.vector for tok in doc])/len(doc)\n",
    "print(document_vector == mean_document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dezambiguacja semantyczna\n",
    "\n",
    "Podstawowym narzędziem służącym do określenia który z sensów wyrazu wieloznacznego występuje w tekście, jest algorytm Leska. W wersji tradycyjnej, opiera się o reprezentację BoW, i zdefiniowane dla nich miary podobieństwa, ale możemy także uogólnić go do postaci działającej dla reprezentacji word2vec.\n",
    "\n",
    "Algorytm porównuje kontekst w jakim znajduje się docelowe słowo, z glosą, wydobytą z bazy sensów. Tutaj miarą dopasowania będzie podobieństwo cosinusowe.\n",
    "\n",
    "## Zadanie 2:\n",
    "W zmiennej wsd_data umieszczono słownik sensów dla wyrazów \"zamek\", \"igła\", \"pilot\", oraz \"klucz\", wyekstrahowany ze Słowosieci.\n",
    "\n",
    "Przygotuj funkcję, która wykorzysta reprezentacje wektorowe zdań (lub dokumentów), do implementacji algorytmu Leska. dla każdego z przykładów, i wskazanego słowa do dezambiguacji, powinna zwrócić sens, o który chodzi w danym kontekście. Skorzystaj z metody .similarity definiowanej dla obiektów typu doc, oraz sent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = requests.get(\"https://raw.githubusercontent.com/ryszardtuora/workshop_resources/main/wsd_data.json\").text\n",
    "\n",
    "wsd_data = json.loads(txt)\n",
    "for lemma in wsd_data:\n",
    "    for variant, gloss in wsd_data[lemma]:\n",
    "        print(lemma, variant, gloss)\n",
    "        \n",
    "sent_to_disamb1 = \"Początkowo zamek miał kształt zbliżony do owalu, \\\n",
    "                   który był otoczony grubymi na dwa metry murami \\\n",
    "                   obwodowymi z blankami, na jego wewnętrznym dziedzińcu\\\n",
    "                   prawdopodobnie znajdowały się drewniane zabudowania.\"\n",
    "\n",
    "sent_to_disamb2 = \"Pielęgniarka nie mogła znaleźć żyły, igła co raz rozcinała skórę.\"\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuPWYncPLyK4"
   },
   "source": [
    "# Część druga - Tagowanie morfosyntaktyczne\n",
    "korzystamy z tagsetu NKJP\n",
    "Nasz tagger to słownikowy tagger Morfeusz2 + dezambiguacja za pomocą neuronowego Toyggera (biLSTM)\n",
    "\n",
    "Każdy token t ma trzy interesujące nas atrybuty: \n",
    "- t.tag_ : klasa gramatyczna według polskiego tagsetu NKJP (http://nkjp.pl/poliqarp/help/ense2.html)\n",
    "- t.pos_ : klasa gramatyczna według międzynarodowego tagsetu UD (mapowana z NKJP)\n",
    "- t._.feats : customowy atrybut odpowiadający cechom morfosyntaktycznym (np. rodzajowi gramatycznemu, lub liczbie), poszczególne wartości cech są oddzielone dwukropkiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8MPXgjqU2LH"
   },
   "outputs": [],
   "source": [
    "txt = \"Nornica prowadzi zmierzchowo-nocny tryb życia, ale wychodzi również za dnia w poszukiwaniu pokarmu.\"\n",
    "doc = nlp(txt) # przetworzenie textu przez pipeline, na wyjściu dostajemy iterowalny obiekty klasy Doc, przechowujący tokeny\n",
    "\n",
    "print(\"{0:15} {1:8} {2:6} {3:15}\\n\".format(\".orth_\", \"NKJP\", \"UD POS\", \"._.feats\"))\n",
    "for t in doc:\n",
    "    print(\"{0:15} {1:8} {2:6} {3:15}\".format(t.orth_, t.tag_, t.pos_, t._.feats)) # wypisujemy interpretację morfosyntaktyczną każdego tokenu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZ2ioo6nW8eo"
   },
   "source": [
    "## Zadanie 3:\n",
    "\n",
    "Charakterystyka stylów literackich wiąże się z proprocjami części mowy w tekście.\n",
    "\n",
    "![alt text](https://github.com/ryszardtuora/workshop_resources/raw/main/czesci_mowy.png)\n",
    "\n",
    "Źródło: Irena Kamińska-Szmaj, *Różnice leksykalne między stylami funkcjonalnymi polszczyzny pisanej: Analiza statystyczna na materiale słownika frekwencyjnego*, 1990.\n",
    "\n",
    "Przetwórz tekst znajdujący się w zmiennej txt, oblicz proporcję czasowników (używając tagów UD), na podstawie tego oszacuj gatunek do którego należy tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2eBs1u9VoMR"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "txt = requests.get(\"https://raw.githubusercontent.com/ryszardtuora/workshop_resources/main/1.txt\").text\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzHlNwhrthsl"
   },
   "source": [
    "### Fleksja\n",
    "\n",
    "Flexer pozwala na odmianę pojedynczych tokenów, do pożądanej charakterystyki morfologicznej. Argumentami do flexera jest słowo do odmiany (string, lub lepiej otagowany token), i przedzielony dwukropkami string znaczników morfosyntaktycznych.\n",
    "\n",
    "Flexer umożliwia np. wypełnianie szablonów tekstów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4bvg3thmuzt"
   },
   "outputs": [],
   "source": [
    "filizanka = \"filiżanka\"\n",
    "flexer = nlp.get_pipe(\"flexer\")\n",
    "\n",
    "tmpl1 = \"Szukam szarej, ceramicznej {}.\".format(flexer.flex(filizanka, \"gen\"))\n",
    "tmpl2 = \"Marzę o szarej, ceramicznej {}.\".format(flexer.flex(filizanka, \"loc\"))\n",
    "tmpl3 = \"Szukam kompletu ceramicznych {}.\".format(flexer.flex(filizanka, \"gen:pl\"))\n",
    "print(tmpl1)\n",
    "print(tmpl2)\n",
    "print(tmpl3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flexer pozwala także na odmianę fraz wielowyrazowych (multi-word expressions - MWE).\n",
    "\n",
    "W tym celu możemy do niego podać string, lub token odpowiadający głowie frazy.\n",
    "Korzystamy z metody .flex_mwe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filizanka2 = \"biała, porcelanowa filiżanka z Chin\"\n",
    "\n",
    "tmpl1 = \"Szukam {}.\".format(flexer.flex_mwe(filizanka2, \"gen\"))\n",
    "tmpl2 = \"Marzę o {}.\".format(flexer.flex_mwe(filizanka2, \"loc\"))\n",
    "tmpl3 = \"Szukam kompletu {}.\".format(flexer.flex_mwe(filizanka2, \"gen:pl\"))\n",
    "\n",
    "print(tmpl1)\n",
    "print(tmpl2)\n",
    "print(tmpl3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 4\n",
    "\n",
    "Zaprojektuj sposób formułowania znaczników morfosyntaktycznych dla luk w szablonach, który określa jaką formę muszą przyjąć frazy, by móc wypełnić te luki.\n",
    "Zaprojektuj funkcję, która pobiera jako argumenty 1. szablon z tak oznakowaną luką, 2. frazę do wypełnienia luki, która odmienia wskazaną frazę, i wkleja ją w miejsce luki. \n",
    "Możesz założyć, że funkcja ma działać wtedy, i tylko wtedy, gdy w szablonie znajduje się dokładnie jedna luka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Przyglądam się ___.\" # w miejsce ___ wstaw zaprojektowany znacznik, który pozwoli na poprawne\n",
    "                                 # uzupełnienie luki\n",
    "phrase = \"czarne, skórzana torebka ze sprzączką\"\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do modelu można załadować także customowy słownik Morfeusza, wzbogacony o słownictwo dziedzinowe.\n",
    "W tym celu należy pobrać komponent tokenizatora (nlp.tokenizer) oraz flexer, i utworzyć dla nich nową instancję Morfeusza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/ryszardtuora/workshop_resources/blob/main/cust_dict.tar.gz?raw=true\n",
    "!mv 'cust_dict.tar.gz?raw=true' cust_dict.tar.gz\n",
    "!tar -xvf cust_dict.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import morfeusz2\n",
    "\n",
    "flexer = nlp.get_pipe(\"flexer\")\n",
    "print(\"Przed: \", flexer.flex(\"sneakersy\", \"gen\"))\n",
    "\n",
    "dict_name = \"cust_dict\"\n",
    "dict_path = \"cust_dict\"\n",
    "morf = morfeusz2.Morfeusz(whitespace = morfeusz2.KEEP_WHITESPACES,\n",
    "                                   expand_tags=True,\n",
    "                                   dict_name=dict_name, dict_path=dict_path)\n",
    "tokenizer = nlp.tokenizer\n",
    "tokenizer.morf = morf\n",
    "flexer.morf = morf\n",
    "print(\"Po:\", flexer.flex(\"sneakersy\", \"gen\"))\n",
    "\n",
    "# zresetujmy model\n",
    "nlp = spacy.load(\"pl_spacy_model_morfeusz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noq6HpCgY4q2"
   },
   "source": [
    "# Część trzecia - Lematyzacja i własności leksykalne\n",
    "Nasz model umożliwia słownikową lematyzację przy pomocy Morfeusza, do dezambiguacji (tutaj np. rozróżnienia między \"Głos zabrały mamy dzieci.\"-> \"mama\" i \"My mamy samochód.\" -> \"mieć\" służy output taggera).\n",
    "\n",
    "Lematyzacja pozwala redukować redundancję informacyjną, i ułatwiać zadania takie jak streszczanie, i przeszukiwanie.\n",
    "\n",
    "Każdy z tokenów dodatkowo jest oznaczony ze względu na pewne własności leksykalne, np. :\n",
    "- t.lemma_ - lemat - forma reprezentatywna danego leksemu\n",
    "- t.is_stop - słowo należy do stoplisty (listy słów występujących najczęściej, a więc najmniej istotnych semantycznie)\n",
    "- t.is_oov - słowo znajduje się poza słownikiem, i.e. embeddingami wykorzystanymi w modelu\n",
    "- t.like_url - token ma strukturę url-a\n",
    "- t.like_num - token jest liczbą\n",
    "- t.is_alpha - token składa się tylko ze znaków alfabetycznych\n",
    "- t.rank - miejse w rankingu częstości słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFmVQkr2ZsKu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt = \"Zdenerwowany gen. Leese mówił przez telefon swym podwładnym walczącym pod Monte Cassino, że rozmawia z nimi ze schronu.\"\n",
    "doc = nlp(txt)\n",
    "print(\"{0:16} {1:16} {2:5} {3:5} {4:20}\\n\".format(\"forma\", \"lemat\", \"OOV\", \"STOP\", \"Częstość\"))\n",
    "for t in doc:\n",
    "    print(\"{0:16} {1:16} {2:5} {3:5} {4:20}\".format(t.orth_, t.lemma_, t.is_oov, t.is_stop, t.rank)) # orth_ to atrybut odpowiadający formie słowa występującej w tekście"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2cBJlSpcziu"
   },
   "source": [
    "## Zadanie 5:\n",
    "\n",
    "1. Przetwórz tekst spod zmiennej txt, \n",
    "2. Przekonwertuj go do listy lematów\n",
    "3. usuń słowa ze stoplisty, oraz interpunkcję, \n",
    "4. wypisz dziesięć najczęściej pojawiających się lematów. \n",
    "\n",
    "Wypróbuj także opcję w której uwzględniamy tylko rzeczowniki.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HlxUtCEqcy3k"
   },
   "outputs": [],
   "source": [
    "txt = requests.get(\"https://raw.githubusercontent.com/ryszardtuora/workshop_resources/main/2.txt\").text\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yev0e8lQmwMm"
   },
   "source": [
    "# Część czwarta - parsowanie zależnościowe\n",
    "\n",
    "spaCy zawiera parser zależnościowy oparty o metodologię transition-based dependency parsing. \n",
    "Interesują nas tu cztery atrybuty:\n",
    " - t.head - link do tokenu będącego nadrzędnikiem tokenu t\n",
    " - t.dep_ - etykieta opisująca rodzaj zależności\n",
    " - t.subtree - generator opisujący poddrzewo rozpięte przez token t\n",
    " - t.children - generator opisujący wszystkich bezpośrednich potomków tokenu t\n",
    " - t.ancestors - generator opisujący wszystkie przechodnie nadrzędniki tokenu t\n",
    "\n",
    "\n",
    "Opis systemu etykiet: https://universaldependencies.org/u/dep/all.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7i9joa5n3ZK"
   },
   "outputs": [],
   "source": [
    "txt = \"Pierwsza wzmianka o Gdańsku pochodzi ze spisanego po łacinie w 999 Żywotu świętego Wojciecha.\"\n",
    "doc = nlp(txt)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "table = []\n",
    "for tok in doc:\n",
    "    tok_dic = {\"form\": tok.orth_, \"label\": tok.dep_, \"head\": tok.head.orth_, \"subtree\": list(tok.subtree), \"ancestors\": list(tok.ancestors)}\n",
    "    table.append(tok_dic)\n",
    "df = pd.DataFrame(table)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6MI0U4Npeqn"
   },
   "source": [
    "### spaCy posiada wbudowaną wizualizację drzew zależnościowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFX-8z48peHV"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JS6zUzTKmN86"
   },
   "source": [
    "### Poniższa funkcja służy łatwej wizualizacji podstawowych własności tokenów z danego tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDcn4tWTLBwz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def table(doc):\n",
    "    table = []\n",
    "    for tok in doc:\n",
    "        tok_dic = {\"form\": tok.orth_, \"lemma\": tok.lemma_, \"tag\": \":\".join([tok.tag_, tok._.feats]), \"dep_label\": tok.dep_, \"dep_head\": tok.head.orth_}\n",
    "        table.append(tok_dic)\n",
    "    return pd.DataFrame(table)\n",
    "\n",
    "txt = \"Wiadomość jest symboliczna, ale oznacza też początek długotrwałego trendu.\\\n",
    " Dochód na mieszkańca z uwzględnieniem realnej mocy nabywczej walut narodowych \\\n",
    " wyniósł w 2019 r. w Rzeczpospolitej Polskiej 33 891 dolarów, nieco więcej, niż w Portugalii \\\n",
    " (33 665 dolarów). Jednak Fundusz przewiduje, że w tym roku portugalska gospodarka\\\n",
    "  będzie się rozwijać w tempie 1,6 proc. wobec 3,1 proc. w przypadku gospodarki \\\n",
    "  polskiej. Nożyce między oboma krajami będą się więc rozwierać.\"\n",
    "\n",
    "doc = nlp(txt)\n",
    "\n",
    "tab = table(doc)\n",
    "\n",
    "print(tab.to_string()) # prosty hack na wypisanie całej tabeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_EwypobSuHQZ"
   },
   "source": [
    "### Czasami interesują nas większe całości niż pojedyncze tokeny, \n",
    "np. rzeczowniki często łączą się w frazy rzeczownikowe, żeby znajdować takie całości w tekście możemy korzystać z Matchera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgH3Y2ybhYxp"
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADP\"}, {\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"NounAdpNoun\", None, pattern) # nazwa, funkcja, wzór\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    toks = doc[start:end]\n",
    "    print(toks) # obiekt typu Span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematyzacja jednostek wielowyrazowych\n",
    "\n",
    "Flexer pozwala także na prostą lematyzację jednostek wielowyrazowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Flexer vs. konkatenacja lematów:\\n\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    phrase = doc[start:end]\n",
    "    phrase_text = phrase.text\n",
    "    toks = [t for t in phrase]\n",
    "    lemmas = [t.lemma_ for t in toks]\n",
    "    lemma_concat = \" \".join(lemmas)\n",
    "    print(\"{} vs. {}\".format(flexer.lemmatize_mwe(phrase_text), lemma_concat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 6\n",
    "\n",
    "Znajdź wszystkie frazy postaci przymiotnik - rzeczownik, i wypisz je w formie występującej w tekście, oraz formie zlematyzowanej.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matcher pozwala także na korzystanie z innych własności tokenów, np. lematów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YG0hKgyWwc4N"
   },
   "source": [
    "### Parser zależnościowy pozwala także na dzielenie dokumentów na zdania w sposób bardziej inteligentny, niż posługując się regułami interpunkcji.\n",
    "Za zdanie uważamy nieprzerwaną sekwencję tokenów które są powiązane relacjami zależnościowymi.\n",
    "Zdania są zapisane w atrybucie doc.sents dokumentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fH4rg0X4wwIB"
   },
   "outputs": [],
   "source": [
    "for s in doc.sents:\n",
    "    print(s)\n",
    "\n",
    "displacy.render(doc, jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6d5JbrSsnhm"
   },
   "source": [
    "# Część czwarta - Rozpoznawanie jednostek nazewniczych (NER)\n",
    "#### Nasz model do spaCy wykorzystuje 6 rodzajów etykiet:\n",
    "- placeName - miejsca antropogeniczne, np. Dania, Londyn\n",
    "- geogName - naturalne miejsca geograficzne, np. Tatry, Kreta\n",
    "- persName - imiona i nazwiska osób, np. J. F. Kennedy, gen. Maczek\n",
    "- orgName - nazwy organizacji, np. NATO, Unia Europejska\n",
    "- date - daty, np. 22 marca 1999, druga połowa kwietnia\n",
    "- time - czas, np. 18:55, pięć po dwunastej\n",
    "\n",
    "#### Nie pozwala na wykrywanie zagnieżdżonych jednostek nazewniczych, np. [placeName: **aleja** [persName: **Piłsudskiego**]]\n",
    "\n",
    "Wykryte wzmianki są przechowywane w atrybucie doc.ents dokumentu, każda z tych wzmianek ma atrybut ent.label_, w którym przechowywana jest jej etykieta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Bkf6kOWt8GK"
   },
   "outputs": [],
   "source": [
    "print(doc, \"\\n\\n\")\n",
    "for e in doc.ents:\n",
    "    print(e, e.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zniWaxZxvYZz"
   },
   "source": [
    "### displaCy pozwala także na wizualizację jednostek nazewniczych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CJOleJmIvfGs"
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cknnUsl1uaKb"
   },
   "source": [
    "W obecnym modelu NER, uwzględnione są \"uniwersalne\" kategorie jednostek nazewniczych, jednak w zależności od zastosowania będziemy najprawdopodobniej potrzebowali innych kategorii - np. osobnej kategorii dla nazw aktów prawodawczych, lub kwot i walut.\n",
    "\n",
    "Bez problemu można zastąpić domyślny model własnym, wytrenowanym przez CLI spaCy na własnych oznakowanych danych. Więcej informacji tu: https://spacy.io/api/cli#train\n",
    "\n",
    "Jednostki które da się wykryć na podstawie reguł, można wykrywać poprzez EntityRuler: https://spacy.io/api/entityruler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBxZzfaX28_M"
   },
   "source": [
    "## Zadanie 7:\n",
    "\n",
    "W zmiennej txt znajduje się dokument historyczny, wydobądź z niego wszystkie *zdania* zawierające daty, i po kolei je wypisz, rozważ możliwe sposoby automatycznego szeregowania dat wyrażonych w różny sposób (po ewentualnym sprowadzeniu ich do kanonicznej postaci). Rozwiązanie tego problemu jest trudne, i istnieją do niego odrębne narzędzia, umożliwiają one np. automatyczną rekonstrukcję chronologii wydarzeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glLD6N0V3Uo2"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 8:\n",
    "\n",
    "W zmiennej txt znajduje się dokument zawierający nazwiska ludzi. Wykryj pojawiające się w nim imiona i nazwiska osób, i zastąp je znacznikiem - tj. poddaj tekst anonimizacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_marker = \"@Anonymized@\"\n",
    "\n",
    "txt = \"\"\"26-latek trzy lata temu jako piłkarz Levante UD strzelił Realowi Madryt gola na Santiago Bernabeu, a dziś wyrasta na jednego z liderów zespołu Marka Papszuna.\n",
    "Lopez w meczu z beniaminkiem najpierw dołożył w polu karnym nogę do podania Marko Poletanovicia, a później pięknie przymierzył z rzutu wolnego. \n",
    "Raków zwyciężył, choć na ławce spotkanie rozpoczął Czech Petr Schwarz, a w ogóle nie zagrał Marcin Cebula, czyli bohaterowie pierwszych kolejek.\n",
    "\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Część Piąta - Zastosowania Różne\n",
    "\n",
    "W tej części pokażemy kilka przykładów wykorzystania spaCy razem z innymi bibliotekami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streszczanie\n",
    "\n",
    "Streszczanie w najprostszej formie polega na wybraniu najistotniejszych zdań z dokumentu.\n",
    "W materiałach znajduje się zescrapowany z Wikipedii artykuł nt. Polityki Gospodarczej, długości 92 zdań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = requests.get(\"https://raw.githubusercontent.com/ryszardtuora/workshop_resources/main/polityka_gospodarcza.txt\").text\n",
    "\n",
    "doc = nlp(txt)\n",
    "sents = list(doc.sents)\n",
    "n_sents = len(sents)\n",
    "print(n_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jednym ze sposobów punktowania zdań jest wykorzystanie algorytmu PageRank\n",
    "\n",
    "W tym celu musimy zinterpretować macierz podobieństwa zdań w dokumencie, jako macierz sąsiedztwa w grafie. Podobieństwo zaś, liczymy jako podobieństwo cosinusowe zdań do siebie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.metrics import pairwise\n",
    "import networkx as nx\n",
    "\n",
    "TOP_N = 5\n",
    "\n",
    "# przygotowanie macierzy podobieństwa\n",
    "vex = numpy.array([sent.vector for sent in sents])\n",
    "sim_matrix = 1 - pairwise.cosine_distances(vex, vex)\n",
    "\n",
    "# zastosowanie algorytmu TextRank\n",
    "sim_graph = nx.from_numpy_matrix(sim_matrix)\n",
    "sent_scores = nx.pagerank(sim_graph)\n",
    "\n",
    "\n",
    "# przygotowanie wyników\n",
    "ranked_sents = sorted(((sent_scores[i], i, sent) for i, sent in enumerate(sents)), reverse=True)\n",
    "top_n = ranked_sents[:TOP_N]\n",
    "ordered_top_n = sorted(top_n, key=lambda x:x[1])\n",
    "summary_sents = [e[2].text for e in ordered_top_n]\n",
    "summary = \" \".join(summary_sents)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aby zobaczyć co pominęliśmy, możemy wykorzystać displacy, i zaznaczyć nasze zdania jako entities, podobne do tych, wykrywanych przez NER.\n",
    "W tym celu utworzymy obiekty klasy Span, i skorzystamy z atrybutów start i end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "ents = []\n",
    "for score, index, sent in top_n:\n",
    "    start = sent.start\n",
    "    end = sent.end\n",
    "    label = \"SUMM\"\n",
    "    span = Span(doc, start, end, label)\n",
    "    ents.append(span)\n",
    "doc.ents = ents # nadpisywanie wykrytych w tekście obiektów\n",
    "displacy.render(doc, style=\"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 9:\n",
    "\n",
    "Zmierz stosunek długości streszczenia względem tekstu pierwotnego. Poeksperymentuj z parametrem TOP_N i oszacuj, dla jakiej wielkości uzyskujemy najlepsze streszczenie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 10:\n",
    "#### Wraz z wzrostem liczby użytych zdań, wzrasta ryzyko pojawienia się redundancji\n",
    "\n",
    "Zastanów się w jaki sposób możemy przefiltrować wybrane zdania, aby uniknąć powtórzeń?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie wyników\n",
    "ranked_sents = sorted(((sent_scores[i], i, sent) for i, sent in enumerate(sents)), reverse=True)\n",
    "top_n = ranked_sents[:TOP_N]\n",
    "\n",
    "#filtrowanie przez klastry\n",
    "sents_by_clusters = {}\n",
    "for score, i, sent in top_n:\n",
    "    label = int(labels[i])\n",
    "    if label not in sents_by_clusters:\n",
    "        sents_by_clusters[label] = (i, sent)\n",
    "    else:\n",
    "        if len(sent) > len(sents_by_clusters[label][1]):\n",
    "            sents_by_clusters[label] = (i, sent)\n",
    "filtered = [sents_by_clusters[label] for label in sents_by_clusters]\n",
    "ordered = sorted(filtered)\n",
    "summary_sents = [e[1].text for e in ordered]\n",
    "summary = \" \".join(summary_sents)\n",
    "print(len(summary_sents))\n",
    "print(summary)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja tekstów\n",
    "\n",
    "Jeżeli chcemy skonstruować potok do klasyfikacji tekstów, spaCy może spełnić w nim różne role.\n",
    "  1. może być źródłem embeddingów\n",
    "  2. może służyć do preprocessingu i czyszczenia danych (tokenizacji, filtrowania, reprezentacji wektorowej)\n",
    "\n",
    "Ale:\n",
    "spaCy ma także wbudowany komponent klasyfikatora tekstów, który możemy sami wyuczyć na naszych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/ryszardtuora/workshop_resources/blob/main/poleval.tar.gz?raw=true\n",
    "!mv 'poleval.tar.gz?raw=true' poleval.tar.gz\n",
    "!tar -xvf poleval.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    off_train = 0\n",
    "    off_test = 0\n",
    "    with open(\"poleval/training_set_clean_only_text.txt\") as f:\n",
    "        txt = f.read()\n",
    "        train_sents = txt.split(\"\\n\")[:-1]\n",
    "    with open(\"poleval/training_set_clean_only_tags.txt\") as f:\n",
    "        txt = f.read()\n",
    "        train_labels = [int(x) for x in txt.split(\"\\n\")[:-1]]\n",
    "    with open(\"poleval/Task6/task 01/test_set_clean_only_text.txt\") as f:\n",
    "        txt = f.read()\n",
    "        test_sents = txt.split(\"\\n\")[:-1]\n",
    "    with open(\"poleval/Task6/task 01/test_set_clean_only_tags.txt\") as f:\n",
    "        txt = f.read()\n",
    "        test_labels = [int(x) for x in txt.split(\"\\n\")[:-1]]\n",
    "\n",
    "    # oversampling\n",
    "    off_sents = []\n",
    "    for x, y in zip(train_sents, train_labels):\n",
    "        if y == 1:\n",
    "            off_sents.append(x)\n",
    "    oversampling_scale = 8\n",
    "    oversampling_x = oversampling_scale * off_sents\n",
    "    oversampling_y = [1 for x in oversampling_x]\n",
    "    train_sents.extend(oversampling_x)\n",
    "    train_labels.extend(oversampling_y)\n",
    "    indices = list(range(len(train_sents)))\n",
    "    random.shuffle(indices)\n",
    "    train_sents = [train_sents[i] for i in indices]\n",
    "    train_labels = [train_labels[i] for i in indices]\n",
    "\n",
    "    train_cats = []\n",
    "    test_cats = []\n",
    "    \n",
    "    ### Przygotowanie anotacji danych dla spaCy\n",
    "    # Anotacje mają postać listy słowników z wartościami logicznymi dla każdej z etykiet\n",
    "    for tl in train_labels:\n",
    "        train_cats.append({\"OFF\": bool(tl), \"NONOFF\": not bool(tl)})\n",
    "        if tl:\n",
    "            off_train += 1\n",
    "    for tl in test_labels:\n",
    "        test_cats.append({\"OFF\": bool(tl), \"NONOFF\": not bool(tl)})\n",
    "        if tl:\n",
    "            off_test += 1\n",
    "    ###\n",
    "    print(\"Proportion of offensive tweets in the train data: {}\".format(off_train/len(train_sents)))\n",
    "    print(\"Proportion of offensive tweets in the test data: {}\".format(off_test/len(test_sents)))\n",
    "    return train_sents, test_sents, train_cats, test_cats\n",
    "\n",
    "train_sents, test_sents, train_cats, test_cats = load_data()\n",
    "print(\"Data loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if label == \"NONOFF\":\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.0\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.0\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "nlp = spacy.load(\"pl_spacy_model_morfeusz\")\n",
    "# Inicjalizacja modelu\n",
    "textcat = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"})\n",
    "nlp.add_pipe(textcat, last=True)\n",
    "textcat.add_label(\"OFF\")\n",
    "textcat.add_label(\"NONOFF\")\n",
    "\n",
    "# Trening\n",
    "n_texts = 1000\n",
    "n_iter = 2\n",
    "train_sents = train_sents[:n_texts]\n",
    "train_cats = train_cats[:n_texts]\n",
    "train_data = list(zip(train_sents, [{\"cats\": cats} for cats in train_cats]))\n",
    "\n",
    "print(len(train_sents))\n",
    "\n",
    "pipe_exceptions = [\"textcat\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "    print(\"Training the model...\")\n",
    "    print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
    "    batch_sizes = compounding(4.0, 32.0, 1.001)\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=batch_sizes)\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            scores = evaluate(nlp.tokenizer, textcat, test_sents, test_cats)\n",
    "        print(\"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table\n",
    "                losses[\"textcat\"],\n",
    "                scores[\"textcat_p\"],\n",
    "                scores[\"textcat_r\"],\n",
    "                scores[\"textcat_f\"],\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wykorzystanie wytrenowanego modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Powieś się debilu.\")\n",
    "print(doc.cats)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zquQAX2HGVpg",
    "PuPWYncPLyK4",
    "noq6HpCgY4q2",
    "yev0e8lQmwMm",
    "E6d5JbrSsnhm",
    "_4a5Izt4Tsae"
   ],
   "name": "Copy of Copy of Copy of Copy of Copy of Gdańsk.ipynb",
   "provenance": [
    {
     "file_id": "1MWiYw3ugMhpq38tBhkbWi1ZMLHU17IqZ",
     "timestamp": 1594737900299
    },
    {
     "file_id": "1lfYKkZyhW-hfQ6b0qsXxrVTjGquz8Qdn",
     "timestamp": 1594727658531
    },
    {
     "file_id": "1onFhNTAHRwnqjbCsYx1BdgoE76O5Ssix",
     "timestamp": 1594667047412
    },
    {
     "file_id": "1APwoUXjdVCZQVF6B1zMJUy5VZPQU1m43",
     "timestamp": 1594650744668
    },
    {
     "file_id": "https://github.com/ryszardtuora/webinar_resources/blob/master/Gda%C5%84sk.ipynb",
     "timestamp": 1594596301441
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
